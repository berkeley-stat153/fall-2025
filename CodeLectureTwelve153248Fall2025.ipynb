{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Bayesian Regularization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the temperature anomalies dataset that we already used in the last lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_jan = pd.read_csv('TempAnomalies_January.csv', skiprows=4)\n",
    "print(temp_jan.head())\n",
    "y = temp_jan['Anomaly']\n",
    "plt.plot(y)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('Celsius')\n",
    "plt.title('Temperature anomalies (from 1901-2000 average) for January')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $X$ matrix for our high-dimensional regression model is calculated as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(y)\n",
    "x = np.arange(1, n+1)\n",
    "X = np.column_stack([np.ones(n), x-1])\n",
    "for i in range(n-2):\n",
    "    c = i+2\n",
    "    xc = ((x > c).astype(float))*(x-c)\n",
    "    X = np.column_stack([X, xc])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior corresponding to Normal Prior\n",
    "\n",
    "Consider the prior $\\beta \\sim N(0, Q)$ along with likelihood given by $N(X \\beta, \\sigma^2 I)$. The posterior of $\\beta$ (given the data and $\\sigma$) is then: \n",
    "\\begin{align*}\n",
    "   \\beta \\mid \\text{data}, \\sigma \\sim N \\left(\\left(\\frac{X^T\n",
    "                          X}{\\sigma^2} + Q^{-1}  \\right)^{-1} \\frac{X^T y}{\\sigma^2},  \\left(\\frac{X^T X}{\\sigma^2} + Q^{-1} \\right)^{-1}\\right). \n",
    "\\end{align*}\n",
    "Given $\\tau$ (and a large positive $C$), we take $Q$ to be the diagonal matrix with diagonals $C, C, \\tau^2, \\dots, \\tau^2$. The code below computes the posterior mean: \n",
    "\\begin{equation*}\n",
    "   \\left(\\frac{X^T\n",
    "                          X}{\\sigma^2} + Q^{-1}  \\right)^{-1} \\frac{X^T y}{\\sigma^2}. \n",
    "\\end{equation*}\n",
    "for fixed $C$, $\\tau$ and $\\sigma$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior mean of beta with fixed tau and sig\n",
    "C = 10**10\n",
    "tau = 0.001\n",
    "#tau = .0001\n",
    "sig = 0.2\n",
    "Q = np.diag(np.concatenate([[C, C], np.repeat(tau**2, n-2)]))\n",
    "\n",
    "XTX = np.dot(X.T, X)\n",
    "TempMat = np.linalg.inv(np.linalg.inv(Q) + (XTX/(sig ** 2)))\n",
    "XTy = np.dot(X.T, y)\n",
    "\n",
    "betahat = np.dot(TempMat, XTy/(sig ** 2))\n",
    "muhat = np.dot(X, betahat)\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(y)\n",
    "plt.plot(muhat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We showed in class that this posterior mean coincides with the ridge regression estimate from last lecture provided the tuning parameter $\\lambda$ in ridge regression is related to $\\tau$ and $\\sigma$ via $\\lambda = \\sigma^2/\\tau^2$. This can be verified as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below penalty_start = 2 means that b0 and b1 are not included in the penalty\n",
    "def solve_ridge(X, y, lambda_val, penalty_start=2):\n",
    "    n, p = X.shape\n",
    "    \n",
    "    # Define variable\n",
    "    beta = cp.Variable(p)\n",
    "    \n",
    "    # Define objective\n",
    "    loss = cp.sum_squares(X @ beta - y)\n",
    "    reg = lambda_val * cp.sum_squares(beta[penalty_start:])\n",
    "    objective = cp.Minimize(loss + reg)\n",
    "    \n",
    "    # Solve problem\n",
    "    prob = cp.Problem(objective)\n",
    "    prob.solve()\n",
    "    \n",
    "    return beta.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ridge = solve_ridge(X, y, lambda_val = (sig ** 2)/(tau ** 2))\n",
    "ridge_fitted = np.dot(X, b_ridge)\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(y, color = 'lightgray', label = 'data')\n",
    "plt.plot(ridge_fitted, color = 'red', label = 'Ridge')\n",
    "plt.plot(muhat, color = 'blue', label = 'Posterior mean')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(np.column_stack((ridge_fitted, muhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next perform posterior inference for all the parameters $\\beta, \\sigma, \\tau$. We follow the method described in lecture. First we take a grid of values of $\\tau$ and $\\sigma$ and compute the posterior (on the logarithmic scale) of $\\tau$ and $\\sigma$. The posterior for $\\tau$ and $\\sigma$ (derived in class) is given by: \n",
    "\\begin{align*}\n",
    "&  f_{\\tau, \\sigma \\mid \\text{data}}(\\tau, \\sigma) \\\\ &\\propto\n",
    "  \\frac{\\sigma^{-n-1} \\tau^{-1}}{\\sqrt{\\det Q}}  \\sqrt{\\det\n",
    "  \\left(\\frac{X^T X}{\\sigma^2} + Q^{-1} \\right)^{-1}} \\exp \\left(-\\frac{y^T y}{2 \\sigma^2} \\right)\\exp\n",
    "  \\left(\\frac{y^T X}{2\\sigma^2} \\left(\\frac{X^T \n",
    "  X}{\\sigma^2} + \n",
    "   Q^{-1} \\right)^{-1} \\frac{X^T y}{\\sigma^2}  \\right). \n",
    "\\end{align*}\n",
    "where $Q$ is the diagonal matrix with diagonal entries $C, C, \\tau^2, \\dots, \\tau^2$. We can simplify this formula slightly in order to avoid specifying $C$ explicitly. Note that\n",
    "\\begin{align*} \n",
    "   \\det Q  = C \\times C \\times \\tau^2 \\times \\dots \\times \\tau^2 = C^2 \\tau^{2(n-2)} \\propto \\tau^{2(n-2)}. \n",
    "\\end{align*}\n",
    "Also\n",
    "\\begin{align*}\n",
    "   Q^{-1} = \\text{diag}(1/C, 1/C, 1/\\tau^2, \\dots, 1/\\tau^2) \\approx \\text{diag}(0, 0, 1/\\tau^2, \\dots, 1/tau^2). \n",
    "\\end{align*}\n",
    "Let $Q^{-1}_{\\text{approx}}$ be the above diagonal matrix with diagonal entries $0, 0, 1/\\tau^2, \\dots, 1/\\tau^2$. We shall use $Q^{-1}_{\\text{approx}}$ as our proxy for $Q^{-1}$. Note that $Q^{-1}_{\\text{approx}}$ does not involve any specification of $C$. With this, we rewrite the posterior of $\\tau, \\sigma$ as: \n",
    "\\begin{align*}\n",
    "&  f_{\\tau, \\sigma \\mid \\text{data}}(\\tau, \\sigma) \\\\ &\\propto\n",
    "  \\sigma^{-n-1} \\tau^{-n+1}   \\sqrt{\\det\n",
    "  \\left(\\frac{X^T X}{\\sigma^2} + Q_{\\text{approx}}^{-1} \\right)^{-1}} \\exp \\left(-\\frac{y^T y}{2 \\sigma^2} \\right)\\exp\n",
    "  \\left(\\frac{y^T X}{2\\sigma^2} \\left(\\frac{X^T \n",
    "  X}{\\sigma^2} + \n",
    "   Q^{-1}_{\\text{approx}} \\right)^{-1} \\frac{X^T y}{\\sigma^2}  \\right). \n",
    "\\end{align*}\n",
    "Below we compute this posterior on log-scale for each value in a grid chosen for $\\tau$ and $\\sigma$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_gr = np.logspace(np.log10(0.0001), np.log10(1), 100)\n",
    "sig_gr = np.logspace(np.log10(0.1), np.log10(1), 100)\n",
    "\n",
    "t, s = np.meshgrid(tau_gr, sig_gr)\n",
    "\n",
    "g = pd.DataFrame({'tau': t.flatten(), 'sig': s.flatten()})\n",
    "\n",
    "for i in range(len(g)):\n",
    "    tau = g.loc[i, 'tau']\n",
    "    sig = g.loc[i, 'sig']\n",
    "    Qinv_approx = np.diag(np.concatenate([[0, 0], np.repeat(tau**(-2), n-2)]))\n",
    "    Mat = Qinv_approx + (X.T @ X)/(sig ** 2)\n",
    "    Matinv = np.linalg.inv(Mat)\n",
    "    sgn, logcovdet = np.linalg.slogdet(Matinv)\n",
    "    g.loc[i, 'logpost'] = (-n-1)*np.log(sig) + (-n+1)*np.log(tau) + 0.5 * logcovdet - ((np.sum(y ** 2))/(2*(sig ** 2))) + (y.T @ X @ Matinv @ X.T @ y)/(2 * (sig ** 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point estimates of $\\tau$ and $\\sigma$ can be obtained either by maximizers of the posterior or posterior means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Posterior maximizers:\n",
    "max_row = g['logpost'].idxmax()\n",
    "print(max_row)\n",
    "tau_opt = g.loc[max_row, 'tau']\n",
    "sig_opt = g.loc[max_row, 'sig']\n",
    "print(tau_opt, sig_opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we fix $\\tau$ and $\\sigma$ to be the posterior maximizers, and then find the posterior mean of $\\beta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior mean of beta with tau_opt and sig_opt\n",
    "C = 10**4\n",
    "tau = tau_opt\n",
    "sig = sig_opt\n",
    "Q = np.diag(np.concatenate([[C, C], np.repeat(tau**2, n-2)]))\n",
    "\n",
    "XTX = np.dot(X.T, X)\n",
    "TempMat = np.linalg.inv(np.linalg.inv(Q) + (XTX/(sig ** 2)))\n",
    "XTy = np.dot(X.T, y)\n",
    "\n",
    "betahat = np.dot(TempMat, XTy/(sig ** 2))\n",
    "muhat = np.dot(X, betahat)\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(y)\n",
    "plt.plot(muhat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we convert the log-posterior values to posterior values (this is the posterior over $\\tau$ and $\\sigma$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g['post'] = np.exp(g['logpost'] - np.max(g['logpost']))\n",
    "g['post'] = g['post']/np.sum(g['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute posterior means of $\\tau$ and $\\sigma$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_pm = np.sum(g['tau'] * g['post'])\n",
    "sig_pm = np.sum(g['sig'] * g['post'])\n",
    "print(tau_pm, tau_opt)\n",
    "print(sig_pm, sig_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior means are quite close to the posterior maximizers obtained previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compute posterior samples of all the parameters $\\beta, \\tau, \\sigma$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "samples = g.sample(N, weights = g['post'], replace = True)\n",
    "tau_samples = np.array(samples.iloc[:,0])\n",
    "sig_samples = np.array(samples.iloc[:,1])\n",
    "betahats = np.zeros((n, N))\n",
    "muhats = np.zeros((n, N))\n",
    "for i in range(N):\n",
    "    tau = tau_samples[i]\n",
    "    sig = sig_samples[i]\n",
    "    Q = np.diag(np.concatenate([[C, C], np.repeat(tau**2, n-2)]))\n",
    "    XTX = np.dot(X.T, X)\n",
    "    TempMat = np.linalg.inv(np.linalg.inv(Q) + (XTX/(sig ** 2)))\n",
    "    XTy = np.dot(X.T, y)\n",
    "    betahat = np.dot(TempMat, XTy/(sig ** 2))\n",
    "    muhat = np.dot(X, betahat)\n",
    "    betahats[:,i] = betahat\n",
    "    muhats[:,i] = muhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these samples, we can obtain approximations of the posterior means of $\\beta$ and the fitted values $X \\beta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_est = np.mean(betahats, axis = 1)\n",
    "mu_est = np.mean(muhats, axis = 1) #these are the fitted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot the fitted values corresponding to the samples of $\\beta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(y, label = 'Data')\n",
    "for i in range(N):\n",
    "    plt.plot(muhats[:,i], color = 'red')\n",
    "plt.plot(mu_est, color = 'black', label = 'Posterior Mean')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting that the uncertainty band is narrow at some time points and wider in others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are histograms of the samples of $\\tau$ and $\\sigma$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(tau_samples, bins=30)\n",
    "plt.title('Histogram of tau samples')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite interesting that the posterior samples of $\\tau$ correspond to values that are neither too large (wiggly function fits) and neither too small (almost-linear function fits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sig_samples, bins=30)\n",
    "plt.title('Histogram of sigma samples')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat153fall2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
